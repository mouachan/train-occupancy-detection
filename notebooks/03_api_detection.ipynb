{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KServe API Person Detection\n",
    "\n",
    "This notebook demonstrates person detection using the YOLO model deployed on OpenShift AI via KServe REST API.\n",
    "\n",
    "## Prerequisites:\n",
    "- ONNX model deployed on OpenShift AI\n",
    "- InferenceService endpoint accessible\n",
    "- Network connectivity to OpenShift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from src.api.kserve_client import KServeClient\n",
    "from src.detection.visualizer import draw_detections, create_detection_summary\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure KServe Endpoint\n",
    "\n",
    "Update the endpoint URL to match your OpenShift AI deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure endpoint (update with your actual endpoint)\n# Get the URL with: oc get inferenceservice -n train-detection\nENDPOINT_URL = \"https://train-detection-model-train-detection.apps.cluster-rk6mx.rk6mx.sandbox492.opentlc.com\"\n\n# IMPORTANT: Use Triton model name, not InferenceService name\n# The InferenceService name is \"train-detection-model\" (from oc get inferenceservice)\n# But the Triton model name is \"yolo11n\" (from S3 directory structure)\nMODEL_NAME = \"yolo11n\"\n\n# For local testing with port-forward:\n# oc port-forward svc/train-detection-model-predictor 8080:8080 -n train-detection\n# ENDPOINT_URL = \"http://localhost:8080\"\n\nprint(f\"Endpoint URL: {ENDPOINT_URL}\")\nprint(f\"Model Name: {MODEL_NAME}\")\nprint(f\"\\nFull inference URL: {ENDPOINT_URL}/v2/models/{MODEL_NAME}/infer\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize KServe Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize client\n",
    "client = KServeClient(\n",
    "    endpoint_url=ENDPOINT_URL,\n",
    "    model_name=MODEL_NAME,\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "print(\"KServe client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if endpoint is healthy\n",
    "is_healthy = client.health_check()\n",
    "\n",
    "if is_healthy:\n",
    "    print(\"✓ Endpoint is healthy\")\n",
    "else:\n",
    "    print(\"✗ Endpoint health check failed\")\n",
    "    print(\"Please verify:\")\n",
    "    print(\"  1. InferenceService is deployed and running\")\n",
    "    print(\"  2. Endpoint URL is correct\")\n",
    "    print(\"  3. Network connectivity to OpenShift cluster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get Model Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model metadata\n",
    "metadata = client.get_metadata()\n",
    "\n",
    "if metadata:\n",
    "    print(\"Model Metadata:\")\n",
    "    print(f\"  Name: {metadata.name}\")\n",
    "    print(f\"  Platform: {metadata.platform}\")\n",
    "    print(f\"  Inputs: {metadata.inputs}\")\n",
    "    print(f\"  Outputs: {metadata.outputs}\")\n",
    "else:\n",
    "    print(\"Could not retrieve model metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Inference on Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample image\n",
    "import urllib.request\n",
    "\n",
    "sample_image_url = \"https://ultralytics.com/images/bus.jpg\"\n",
    "sample_image_path = \"sample_image.jpg\"\n",
    "\n",
    "urllib.request.urlretrieve(sample_image_url, sample_image_path)\n",
    "print(f\"Downloaded sample image to: {sample_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference via API\n",
    "result = client.predict_from_file(sample_image_path, conf_threshold=0.25)\n",
    "\n",
    "print(f\"Inference Time: {result.inference_time_ms:.2f} ms\")\n",
    "print(f\"Model Name: {result.model_name}\")\n",
    "print(f\"Image Shape: {result.image_shape}\")\n",
    "print(f\"Number of persons detected: {len(result.detections)}\")\n",
    "\n",
    "# Print detection details\n",
    "for i, det in enumerate(result.detections):\n",
    "    print(f\"Person {i+1}: confidence={det.confidence:.2f}, bbox={[round(x, 1) for x in det.bbox]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "image = cv2.imread(sample_image_path)\n",
    "annotated_image = draw_detections(image, result.detections)\n",
    "annotated_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.imshow(annotated_rgb)\n",
    "plt.title(f\"API Detection: {len(result.detections)} persons - {result.inference_time_ms:.2f} ms\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare with Local Model\n",
    "\n",
    "Compare API inference results with local YOLOv11 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.detection.yolo_detector import YOLODetector\n",
    "from src.utils.config import Config\n",
    "\n",
    "# Load local detector\n",
    "local_detector = YOLODetector(\n",
    "    model_path=str(Config.get_model_path('yolo11n.pt')),\n",
    "    conf_threshold=0.25\n",
    ")\n",
    "\n",
    "# Run local inference\n",
    "start_time = time.time()\n",
    "local_image, local_detections = local_detector.process_image(sample_image_path)\n",
    "local_time = (time.time() - start_time) * 1000\n",
    "\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"  API Detections: {len(result.detections)} in {result.inference_time_ms:.2f} ms\")\n",
    "print(f\"  Local Detections: {len(local_detections)} in {local_time:.2f} ms\")\n",
    "print(f\"  Difference: {abs(len(result.detections) - len(local_detections))} detections\")\n",
    "print(f\"  Time Overhead: {(result.inference_time_ms - local_time):.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Benchmark API Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark API inference\n",
    "num_runs = 10\n",
    "times = []\n",
    "detection_counts = []\n",
    "\n",
    "print(f\"Running {num_runs} API inference requests...\")\n",
    "for i in range(num_runs):\n",
    "    result = client.predict_from_file(sample_image_path)\n",
    "    times.append(result.inference_time_ms)\n",
    "    detection_counts.append(len(result.detections))\n",
    "    print(f\"  Run {i+1}: {result.inference_time_ms:.2f} ms, {len(result.detections)} detections\")\n",
    "\n",
    "print(f\"\\nBenchmark Results ({num_runs} runs):\")\n",
    "print(f\"  Average Time: {np.mean(times):.2f} ms\")\n",
    "print(f\"  Min Time: {np.min(times):.2f} ms\")\n",
    "print(f\"  Max Time: {np.max(times):.2f} ms\")\n",
    "print(f\"  Std Dev: {np.std(times):.2f} ms\")\n",
    "print(f\"  Throughput: {1000/np.mean(times):.2f} requests/sec\")\n",
    "print(f\"  Detection Consistency: {np.std(detection_counts):.2f} std dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Multiple Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download additional test images\n",
    "test_images = [\n",
    "    \"https://ultralytics.com/images/bus.jpg\",\n",
    "    \"https://ultralytics.com/images/zidane.jpg\"\n",
    "]\n",
    "\n",
    "for idx, img_url in enumerate(test_images):\n",
    "    img_path = f\"test_image_{idx}.jpg\"\n",
    "    urllib.request.urlretrieve(img_url, img_path)\n",
    "    \n",
    "    # Run inference\n",
    "    result = client.predict_from_file(img_path)\n",
    "    \n",
    "    # Visualize\n",
    "    image = cv2.imread(img_path)\n",
    "    annotated = draw_detections(image, result.detections)\n",
    "    annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(annotated_rgb)\n",
    "    plt.title(f\"Image {idx+1}: {len(result.detections)} persons - {result.inference_time_ms:.2f} ms\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    summary = create_detection_summary(result.detections)\n",
    "    print(f\"\\nImage {idx+1} Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Connecting to KServe model endpoint on OpenShift AI\n",
    "- Health checking and metadata retrieval\n",
    "- Running inference via REST API\n",
    "- Comparing API vs local inference\n",
    "- Performance benchmarking\n",
    "\n",
    "The API adds network overhead but provides:\n",
    "- Centralized model serving\n",
    "- Auto-scaling capabilities\n",
    "- Version management\n",
    "- Multi-client access"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}