{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Triton API Person Detection\n\nThis notebook demonstrates person detection using YOLOv11 deployed on OpenShift AI with Triton Inference Server via direct HTTP/REST API calls.\n\n## Prerequisites:\n- ONNX model deployed on OpenShift AI with Triton runtime\n- InferenceService endpoint accessible\n- Network connectivity to OpenShift cluster"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '..')\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nimport time\nimport requests\nimport urllib3\n\n# Disable SSL warnings for self-signed certificates\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\nfrom src.detection.visualizer import draw_detections, create_detection_summary\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (12, 8)\n\nprint(\"Libraries imported successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Configure Triton Endpoint\n\nUpdate the endpoint URL to match your OpenShift AI deployment."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure endpoint (update with your actual endpoint)\n# Get the URL with: oc get inferenceservice -n train-detection\nENDPOINT_URL = \"https://train-detection-model-train-detection.apps.cluster-rk6mx.rk6mx.sandbox492.opentlc.com\"\n\n# IMPORTANT: Use Triton model name, not InferenceService name\n# The InferenceService name is \"train-detection-model\" (from oc get inferenceservice)\n# But the Triton model name is \"yolo11n\" (from S3 directory structure)\nMODEL_NAME = \"yolo11n\"\n\n# Confidence threshold for detections\nCONF_THRESHOLD = 0.25\n\n# API endpoints\nHEALTH_URL = f\"{ENDPOINT_URL}/v2/health/ready\"\nMETADATA_URL = f\"{ENDPOINT_URL}/v2/models/{MODEL_NAME}\"\nINFER_URL = f\"{ENDPOINT_URL}/v2/models/{MODEL_NAME}/infer\"\n\nprint(f\"Endpoint URL: {ENDPOINT_URL}\")\nprint(f\"Model Name: {MODEL_NAME}\")\nprint(f\"\\nAPI Endpoints:\")\nprint(f\"  Health: {HEALTH_URL}\")\nprint(f\"  Metadata: {METADATA_URL}\")\nprint(f\"  Inference: {INFER_URL}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Helper Functions for Inference\n\nFunctions for preprocessing images, calling Triton API, and postprocessing results."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def preprocess_image(image_path, target_size=640):\n    \"\"\"Preprocess image for YOLOv11 inference.\"\"\"\n    # Read image\n    img = cv2.imread(str(image_path))\n    if img is None:\n        raise ValueError(f\"Could not read image: {image_path}\")\n    \n    orig_shape = img.shape[:2]  # H, W\n    \n    # Resize to target size (letterbox)\n    img_resized = cv2.resize(img, (target_size, target_size))\n    \n    # Convert BGR to RGB\n    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n    \n    # Normalize to [0, 1]\n    img_normalized = img_rgb.astype(np.float32) / 255.0\n    \n    # Transpose from HWC to CHW\n    img_chw = np.transpose(img_normalized, (2, 0, 1))\n    \n    # Add batch dimension [3,640,640] -> [1,3,640,640]\n    # Required when model has max_batch_size > 0\n    img_batch = np.expand_dims(img_chw, axis=0)\n    \n    return img_batch, orig_shape\n\n\ndef postprocess_yolo_output(output_data, orig_shape, conf_threshold=0.25, input_size=640):\n    \"\"\"Postprocess YOLO output to extract person detections.\"\"\"\n    # YOLO output shape: [1, 84, 8400]\n    # 84 = 4 bbox coords + 80 class scores\n    # 8400 = number of predictions\n    \n    # Convert list to numpy array and reshape\n    if isinstance(output_data, list):\n        output_data = np.array(output_data)\n    \n    # Reshape from flat array [705600] to [1, 84, 8400]\n    if len(output_data.shape) == 1:\n        output_data = output_data.reshape(1, 84, 8400)\n    \n    # Remove batch dimension [1, 84, 8400] -> [84, 8400]\n    if len(output_data.shape) == 3:\n        output_data = output_data[0]\n    \n    # Transpose to [8400, 84]\n    if output_data.shape[0] == 84:\n        output_data = output_data.T\n    \n    # Extract bboxes and scores\n    boxes = output_data[:, :4]  # x, y, w, h\n    scores = output_data[:, 4:]  # class scores\n    \n    # Get person class (class 0 in COCO)\n    person_scores = scores[:, 0]\n    \n    # Filter by confidence\n    mask = person_scores > conf_threshold\n    boxes = boxes[mask]\n    person_scores = person_scores[mask]\n    \n    # Scale boxes to original image size\n    scale_x = orig_shape[1] / input_size\n    scale_y = orig_shape[0] / input_size\n    \n    detections = []\n    for box, score in zip(boxes, person_scores):\n        x, y, w, h = box\n        \n        # Convert from center format to corner format\n        x1 = (x - w/2) * scale_x\n        y1 = (y - h/2) * scale_y\n        x2 = (x + w/2) * scale_x\n        y2 = (y + h/2) * scale_y\n        \n        detections.append({\n            'bbox': [float(x1), float(y1), float(x2), float(y2)],\n            'confidence': float(score),\n            'class_id': 0,\n            'class_name': 'person'\n        })\n    \n    return detections\n\n\ndef infer_call(image_path, conf_threshold=0.25, debug=False):\n    \"\"\"Generic inference call - works with any KServe V2 compatible backend.\"\"\"\n    start_time = time.time()\n    \n    # Preprocess\n    img_data, orig_shape = preprocess_image(image_path)\n    \n    # Prepare request (KServe V2 format)\n    request_data = {\n        \"inputs\": [{\n            \"name\": \"images\",\n            \"shape\": list(img_data.shape),\n            \"datatype\": \"FP32\",\n            \"data\": img_data.flatten().tolist()\n        }]\n    }\n    \n    if debug:\n        print(\"=== DEBUG: Request ===\")\n        print(f\"Image shape: {img_data.shape}\")\n        print(f\"Request shape: {request_data['inputs'][0]['shape']}\")\n        print(f\"Data length: {len(request_data['inputs'][0]['data'])}\")\n        print(f\"URL: {INFER_URL}\")\n    \n    # Send request\n    response = requests.post(\n        INFER_URL,\n        json=request_data,\n        verify=False,\n        timeout=30\n    )\n    \n    # Handle errors\n    if response.status_code != 200:\n        print(f\"=== ERROR {response.status_code} ===\")\n        print(f\"Response: {response.text}\")\n        response.raise_for_status()\n    \n    # Parse response\n    result = response.json()\n    \n    if debug:\n        print(\"\\n=== DEBUG: Response ===\")\n        print(f\"Status: {response.status_code}\")\n        print(f\"Outputs: {len(result.get('outputs', []))}\")\n        if result.get('outputs'):\n            print(f\"Output shape: {result['outputs'][0].get('shape', 'N/A')}\")\n            print(f\"Output data length: {len(result['outputs'][0].get('data', []))}\")\n    \n    output_data = result[\"outputs\"][0][\"data\"]\n    \n    # Postprocess\n    detections = postprocess_yolo_output(output_data, orig_shape, conf_threshold)\n    \n    inference_time = (time.time() - start_time) * 1000\n    \n    return {\n        'detections': detections,\n        'inference_time_ms': inference_time,\n        'orig_shape': orig_shape\n    }\n\nprint(\"Helper functions defined successfully\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Health Check"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if endpoint is healthy\ntry:\n    response = requests.get(HEALTH_URL, verify=False, timeout=10)\n    is_healthy = response.status_code == 200\n    \n    if is_healthy:\n        print(\"✓ Endpoint is healthy\")\n    else:\n        print(f\"✗ Endpoint returned status code: {response.status_code}\")\nexcept Exception as e:\n    print(f\"✗ Health check failed: {e}\")\n    print(\"Please verify:\")\n    print(\"  1. InferenceService is deployed and running\")\n    print(\"  2. Endpoint URL is correct\")\n    print(\"  3. Network connectivity to OpenShift cluster\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Get Model Metadata"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get model metadata\ntry:\n    response = requests.get(METADATA_URL, verify=False, timeout=10)\n    response.raise_for_status()\n    \n    metadata = response.json()\n    \n    print(\"Model Metadata:\")\n    print(f\"  Name: {metadata['name']}\")\n    print(f\"  Platform: {metadata.get('platform', 'N/A')}\")\n    print(f\"  Versions: {metadata.get('versions', [])}\")\n    print(f\"\\n  Inputs:\")\n    for inp in metadata.get('inputs', []):\n        print(f\"    - {inp['name']}: {inp['datatype']} {inp['shape']}\")\n    print(f\"\\n  Outputs:\")\n    for out in metadata.get('outputs', []):\n        print(f\"    - {out['name']}: {out['datatype']} {out['shape']}\")\n        \nexcept Exception as e:\n    print(f\"Could not retrieve model metadata: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Test Inference on Sample Image"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample image\n",
    "import urllib.request\n",
    "\n",
    "sample_image_url = \"https://ultralytics.com/images/bus.jpg\"\n",
    "sample_image_path = \"sample_image.jpg\"\n",
    "\n",
    "urllib.request.urlretrieve(sample_image_url, sample_image_path)\n",
    "print(f\"Downloaded sample image to: {sample_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Run inference via API (with debug enabled first time)\nresult = infer_call(sample_image_path, conf_threshold=CONF_THRESHOLD, debug=True)\n\nprint(f\"\\nInference Time: {result['inference_time_ms']:.2f} ms\")\nprint(f\"Image Shape: {result['orig_shape']}\")\nprint(f\"Number of persons detected: {len(result['detections'])}\")\n\n# Print detection details\nfor i, det in enumerate(result['detections']):\n    bbox = det['bbox']\n    print(f\"Person {i+1}: confidence={det['confidence']:.2f}, bbox=[{bbox[0]:.1f}, {bbox[1]:.1f}, {bbox[2]:.1f}, {bbox[3]:.1f}]\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize results\nimage = cv2.imread(sample_image_path)\n\n# Convert detections to format expected by visualizer\nclass Detection:\n    def __init__(self, bbox, confidence, class_name='person'):\n        self.bbox = bbox\n        self.confidence = confidence\n        self.class_name = class_name\n\ndetections_obj = [Detection(d['bbox'], d['confidence'], d['class_name']) for d in result['detections']]\nannotated_image = draw_detections(image, detections_obj)\nannotated_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(14, 10))\nplt.imshow(annotated_rgb)\nplt.title(f\"API Detection: {len(result['detections'])} persons - {result['inference_time_ms']:.2f} ms\")\nplt.axis('off')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Compare with Local Model\n\nCompare API inference results with local YOLOv11 model."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from src.detection.yolo_detector import YOLODetector\nfrom src.utils.config import Config\n\n# Load local detector\nlocal_detector = YOLODetector(\n    model_path=str(Config.get_model_path('yolo11n.pt')),\n    conf_threshold=CONF_THRESHOLD\n)\n\n# Run local inference\nstart_time = time.time()\nlocal_image, local_detections = local_detector.process_image(sample_image_path)\nlocal_time = (time.time() - start_time) * 1000\n\nprint(\"\\nComparison:\")\nprint(f\"  API Detections: {len(result['detections'])} in {result['inference_time_ms']:.2f} ms\")\nprint(f\"  Local Detections: {len(local_detections)} in {local_time:.2f} ms\")\nprint(f\"  Difference: {abs(len(result['detections']) - len(local_detections))} detections\")\nprint(f\"  Time Overhead: {(result['inference_time_ms'] - local_time):.2f} ms\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Benchmark API Performance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Benchmark API inference\nnum_runs = 10\ntimes = []\ndetection_counts = []\n\nprint(f\"Running {num_runs} API inference requests...\")\nfor i in range(num_runs):\n    result_bench = infer_call(sample_image_path, conf_threshold=CONF_THRESHOLD)\n    times.append(result_bench['inference_time_ms'])\n    detection_counts.append(len(result_bench['detections']))\n    print(f\"  Run {i+1}: {result_bench['inference_time_ms']:.2f} ms, {len(result_bench['detections'])} detections\")\n\nprint(f\"\\nBenchmark Results ({num_runs} runs):\")\nprint(f\"  Average Time: {np.mean(times):.2f} ms\")\nprint(f\"  Min Time: {np.min(times):.2f} ms\")\nprint(f\"  Max Time: {np.max(times):.2f} ms\")\nprint(f\"  Std Dev: {np.std(times):.2f} ms\")\nprint(f\"  Throughput: {1000/np.mean(times):.2f} requests/sec\")\nprint(f\"  Detection Consistency: {np.std(detection_counts):.2f} std dev\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Test Multiple Images"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download additional test images\ntest_images = [\n    \"https://ultralytics.com/images/bus.jpg\",\n    \"https://ultralytics.com/images/zidane.jpg\"\n]\n\n# Detection wrapper class for visualizer\nclass Detection:\n    def __init__(self, bbox, confidence, class_name='person'):\n        self.bbox = bbox\n        self.confidence = confidence\n        self.class_name = class_name\n\nfor idx, img_url in enumerate(test_images):\n    img_path = f\"test_image_{idx}.jpg\"\n    urllib.request.urlretrieve(img_url, img_path)\n    \n    # Run inference\n    result_img = infer_call(img_path, conf_threshold=CONF_THRESHOLD)\n    \n    # Visualize\n    image = cv2.imread(img_path)\n    detections_obj = [Detection(d['bbox'], d['confidence'], d['class_name']) for d in result_img['detections']]\n    annotated = draw_detections(image, detections_obj)\n    annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n    \n    plt.figure(figsize=(10, 7))\n    plt.imshow(annotated_rgb)\n    plt.title(f\"Image {idx+1}: {len(result_img['detections'])} persons - {result_img['inference_time_ms']:.2f} ms\")\n    plt.axis('off')\n    plt.show()\n    \n    summary = create_detection_summary(detections_obj)\n    print(f\"\\nImage {idx+1} Summary: {summary}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9. Test Video Processing via API\n\nProcess a video file frame-by-frame using the API endpoint."
  },
  {
   "cell_type": "code",
   "source": "# Download sample video (or use your own)\nimport urllib.request\n\nvideo_url = \"https://github.com/intel-iot-devkit/sample-videos/raw/master/people-detection.mp4\"\nvideo_path = \"test_video.mp4\"\n\nprint(\"Downloading sample video...\")\nurllib.request.urlretrieve(video_url, video_path)\nprint(f\"✓ Video downloaded: {video_path}\")\n\n# Open video\ncap = cv2.VideoCapture(video_path)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nprint(f\"\\nVideo info:\")\nprint(f\"  Total frames: {total_frames}\")\nprint(f\"  FPS: {fps:.2f}\")\nprint(f\"  Duration: {total_frames/fps:.2f} seconds\")\ncap.release()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize sample frames from video with detections\nNUM_SAMPLES = 3  # Show 3 sample frames\n\ncap = cv2.VideoCapture(video_path)\ntotal_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n# Sample frames at different positions\nsample_positions = [total_frames // 4, total_frames // 2, 3 * total_frames // 4]\n\nfor idx, pos in enumerate(sample_positions):\n    cap.set(cv2.CAP_PROP_POS_FRAMES, pos)\n    ret, frame = cap.read()\n    \n    if not ret:\n        continue\n    \n    # Save and process frame\n    cv2.imwrite(temp_frame_path, frame)\n    result_frame = infer_call(temp_frame_path, conf_threshold=CONF_THRESHOLD)\n    \n    # Annotate\n    detections_obj = [Detection(d['bbox'], d['confidence'], d['class_name']) for d in result_frame['detections']]\n    annotated = draw_detections(frame, detections_obj)\n    annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n    \n    # Display\n    plt.figure(figsize=(12, 8))\n    plt.imshow(annotated_rgb)\n    plt.title(f\"Frame {pos}/{total_frames}: {len(result_frame['detections'])} persons - {result_frame['inference_time_ms']:.2f} ms\")\n    plt.axis('off')\n    plt.show()\n\ncap.release()\n\n# Clean up\nif os.path.exists(temp_frame_path):\n    os.remove(temp_frame_path)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Process video frames via API\n# Process every Nth frame to reduce API calls\nFRAME_SKIP = 5  # Process every 5th frame\n\ncap = cv2.VideoCapture(video_path)\nframe_count = 0\nprocessed_count = 0\ninference_times = []\ndetection_counts = []\ntemp_frame_path = \"temp_frame.jpg\"\n\nprint(f\"Processing video (every {FRAME_SKIP} frames)...\\n\")\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    frame_count += 1\n    \n    # Process every Nth frame\n    if frame_count % FRAME_SKIP != 0:\n        continue\n    \n    # Save frame temporarily\n    cv2.imwrite(temp_frame_path, frame)\n    \n    # Run inference via API\n    try:\n        result_frame = infer_call(temp_frame_path, conf_threshold=CONF_THRESHOLD)\n        inference_times.append(result_frame['inference_time_ms'])\n        detection_counts.append(len(result_frame['detections']))\n        processed_count += 1\n        \n        if processed_count % 10 == 0:\n            print(f\"Frame {frame_count}: {len(result_frame['detections'])} persons detected in {result_frame['inference_time_ms']:.2f} ms\")\n    \n    except Exception as e:\n        print(f\"Error processing frame {frame_count}: {e}\")\n        break\n\ncap.release()\n\n# Clean up temp file\nimport os\nif os.path.exists(temp_frame_path):\n    os.remove(temp_frame_path)\n\nprint(f\"\\n=== Video Processing Results ===\")\nprint(f\"Total frames: {frame_count}\")\nprint(f\"Processed frames: {processed_count}\")\nprint(f\"Average inference time: {np.mean(inference_times):.2f} ms\")\nprint(f\"Average detections per frame: {np.mean(detection_counts):.2f}\")\nprint(f\"Max detections in single frame: {np.max(detection_counts)}\")\nprint(f\"Processing throughput: {1000/np.mean(inference_times):.2f} FPS (API limited)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\nThis notebook demonstrated:\n- Connecting to model endpoint on OpenShift AI (Triton/OpenVINO/any KServe V2 backend)\n- Health checking and metadata retrieval via HTTP\n- Running inference via direct REST API calls (no complex client wrapper)\n- Comparing API vs local inference performance\n- Performance benchmarking on single images\n- **Video processing frame-by-frame via API** (real use case for train monitoring)\n\n**Key Advantages of Direct HTTP + Generic infer_call()**:\n- ✅ **Transparent**: See exactly what's sent/received with debug mode\n- ✅ **Debuggable**: Clear error messages from backend\n- ✅ **Flexible**: Works with any KServe V2 backend (just change URL)\n- ✅ **Simple**: No complex client abstraction layer\n- ✅ **Production-ready**: Tested on video streams (train occupancy use case)\n\nThe API adds network overhead but provides:\n- Centralized model serving\n- Auto-scaling capabilities (1-3 replicas)\n- Version management\n- Multi-client access\n- GPU acceleration (when configured)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}